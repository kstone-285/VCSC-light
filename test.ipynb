{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c99d7cae-43fb-49c5-81cc-33b7ef4d2be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75699f2812664ae3b667a5a4729e4505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The person wearing the backpack appears to be the individual in the foreground on the right side of the image, facing away from the camera. The backpack is visible on their back.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-7B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# default processer\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "\n",
    "# The default range for the number of visual tokens per image in the model is 4-16384.\n",
    "# You can set min_pixels and max_pixels according to your needs, such as a token range of 256-1280, to balance performance and cost.\n",
    "# min_pixels = 256*28*28\n",
    "# max_pixels = 1280*28*28\n",
    "# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "\n",
    "messages = [\n",
    "    # Example 1\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": \"gqa/2354919.jpg\"},\n",
    "            {\"type\": \"text\", \"text\": \"Who is riding a bike? Please provide the bounding box coordinate like [x_min, y_min, x_max, y_max].\"}\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"The man is riding a bike. The bounding box coordinates are [56, 272, 88, 319]. Reasoning: Identify the bike, associate the person riding it, then name the person.\"\n",
    "    },\n",
    "\n",
    "    # Example 2\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": \"gqa/2334016.jpg\"},\n",
    "            {\"type\": \"text\", \"text\": \"What animal is to the right of the man? Please provide the bounding box coordinate like [x_min, y_min, x_max, y_max].\"}\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"The animal is a horse. The bounding box coordinates are [339, 161, 419, 237]. Reasoning: Find the man, locate the animal to his right, determine species.\"\n",
    "    },\n",
    "\n",
    "    # Example 3\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": \"gqa/2415528.jpg\"},\n",
    "            {\"type\": \"text\", \"text\": \"The pot is where? Please provide the bounding box coordinate like [x_min, y_min, x_max, y_max].\"}\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"The pot is on the floor. The bounding box coordinates are [5, 71, 497, 374]. Reasoning: Identify the pot, find its spatial relation, name the location.\"\n",
    "    },\n",
    "\n",
    "    # Example 4\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": \"gqa/2414884.jpg\"},\n",
    "            {\"type\": \"text\", \"text\": \"What is the man watching, a sheep or a donkey? Please provide the bounding box coordinate like [x_min, y_min, x_max, y_max].\"}\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"The man is watching a sheep. The bounding box coordinates are [299, 124, 310, 135]. Reasoning: Identify the man, see which animal he watches, compare sheep and donkey features.\"\n",
    "    },\n",
    "\n",
    "    # Example 5\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": \"gqa/2391800.jpg\"},\n",
    "            {\"type\": \"text\", \"text\": \"What animal is to the right of the man? Please provide the bounding box coordinate like [x_min, y_min, x_max, y_max].\"}\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"The animal is a dog. The bounding box coordinates are [169, 82, 250, 182]. Reasoning: Identify the man, locate animal to the right, determine its type.\"\n",
    "    },\n",
    "\n",
    "    # test query\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": \"gqa/2355243.jpg\"},\n",
    "            {\"type\": \"text\", \"text\": \"Who is wearing the backpack? Please provide the bounding box coordinate of the region that can help you answer the question better.\"}\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=100)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7acf6892-b333-467d-894c-59fbbe252c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Answer: The man is wearing the backpack.\\nCoT BBox: [304, 112, 426, 314]']\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    # Example 1\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": \"gqa/2354919.jpg\"},\n",
    "            {\"type\": \"text\", \"text\": \"Who is riding a bike? Please provide the answer and CoT BBox like [x_min, y_min, x_max, y_max].\"}\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Answer: The man is riding a bike.\\nCoT BBox: [56, 272, 88, 319]\"\n",
    "    },\n",
    "\n",
    "    # Example 2\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": \"gqa/2334016.jpg\"},\n",
    "            {\"type\": \"text\", \"text\": \"What animal is to the right of the man? Please provide the answer and CoT BBox like [x_min, y_min, x_max, y_max].\"}\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Answer: The animal is a horse.\\nCoT BBox: [339, 161, 419, 237]\"\n",
    "    },\n",
    "\n",
    "    # Example 3\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": \"gqa/2415528.jpg\"},\n",
    "            {\"type\": \"text\", \"text\": \"The pot is where? Please provide the answer and CoT BBox like [x_min, y_min, x_max, y_max].\"}\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Answer: The pot is on the floor.\\nCoT BBox: [5, 71, 497, 374]\"\n",
    "    },\n",
    "\n",
    "    # Example 4\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": \"gqa/2414884.jpg\"},\n",
    "            {\"type\": \"text\", \"text\": \"What is the man watching, a sheep or a donkey? Please provide the answer and CoT BBox like [x_min, y_min, x_max, y_max].\"}\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Answer: The man is watching a sheep.\\nCoT BBox: [299, 124, 310, 135]\"\n",
    "    },\n",
    "\n",
    "    # Example 5\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": \"gqa/2391800.jpg\"},\n",
    "            {\"type\": \"text\", \"text\": \"What animal is to the right of the man? Please provide the answer and CoT BBox like [x_min, y_min, x_max, y_max].\"}\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Answer: The animal is a dog.\\nCoT BBox: [169, 82, 250, 182]\"\n",
    "    },\n",
    "\n",
    "    # test query\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": \"gqa/2355243.jpg\"},\n",
    "            {\"type\": \"text\", \"text\": \"Who is wearing the backpack? Please provide the answer and CoT BBox like [x_min, y_min, x_max, y_max].\"}\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=100)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cb503f-e230-44c1-a62d-fec4afe71dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alfworld",
   "language": "python",
   "name": "alfworld"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
